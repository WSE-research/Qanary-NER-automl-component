{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 297,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DZnvzozUAyX",
        "outputId": "25cd6350-9a9d-460b-ac70-3d6ea35f5dcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import gspread\n",
        "import pandas as pd\n",
        "import google.auth\n",
        "import time\n",
        "import gspread_dataframe as gd\n",
        "from google.colab import drive\n",
        "from google.auth import default\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from googleapiclient.discovery import build\n",
        "\n",
        "auth.authenticate_user()\n",
        "drive.mount(\"/drive\")\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "overpass_url = \"http://overpass-api.de/api/interpreter\"\n",
        "\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "gdrive = GoogleDrive(gauth)\n",
        "creds, _ = google.auth.default()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust as needed\n",
        "# The folder the results are saved to:\n",
        "output_folder = \"1WzGGh_Xv23YlrBWHgunrecoicbjSMEoS\"\n",
        "# The folder containing the subfolders for each size of datasets; the subfolders contain the results of each fold (find an example at xxx):\n",
        "search_folder_id = \"17vAQZmIDpAw39HL-F_ix_fWxN3-H3gdZ\"\n",
        "# The ID of the spreadsheet containing the calculation template\n",
        "calc_templates_ssid = \"1MyPUXrpzQbnITkJQ7ZSbKNRXyLyboF0Y9Lrbs9xGpS0\"\n",
        "# The name of the worksheet with the calculations for the F1 score (empty columns are ignored!) \n",
        "calc_template_name = \"BertEnNameTemplates\"\n",
        "# Shows the position of the F1 Scores of each entity + average on the worksheets\n",
        "cell_positions_of_averages = {\n",
        "    \"First_Name\": \"AH2\",\n",
        "    \"Middle_Name\": \"AH3\",\n",
        "    \"Last_Name\": \"AH4\",\n",
        "    \"Average\": \"AH5\"\n",
        "}"
      ],
      "metadata": {
        "id": "u8mvFxBSZc7l"
      },
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate the calculations for the F1 scores for each file type\n",
        "templatesheet = gc.open_by_key(calc_templates_ssid)\n",
        "template_worksheet = templatesheet.worksheet(calc_template_name)\n",
        "calculation= gd.get_as_dataframe(template_worksheet, evaluate_formulas=False)\n",
        "calculation.dropna(how='all', axis=1, inplace=True)\n",
        "calculation.dropna(how='all', inplace=True)"
      ],
      "metadata": {
        "id": "u15U9neRJGLQ"
      },
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_file(folder_id, mimetype):\n",
        "    \"\"\"Search file in drive location\n",
        "\n",
        "    Load pre-authorized user credentials from the environment.\n",
        "    TODO(developer) - See https://developers.google.com/identity\n",
        "    for guides on implementing OAuth2 for the application.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # create drive api client\n",
        "        service = build('drive', 'v3', credentials=creds)\n",
        "        files = []\n",
        "        page_token = None\n",
        "        while True:\n",
        "            # pylint: disable=maybe-no-member\n",
        "            response = service.files().list(q = \"'\" + folder_id + \"' in parents and mimeType = '\" + mimetype + \"'\",\n",
        "                                            spaces='drive',\n",
        "                                            fields='nextPageToken, '\n",
        "                                                   'files(id, name)',\n",
        "                                            pageToken=page_token).execute()\n",
        "            for file in response.get('files', []):\n",
        "                # Process change\n",
        "                print(F'Found: {file.get(\"name\")}, {file.get(\"id\")}')\n",
        "\n",
        "            files.extend(response.get('files', []))\n",
        "            page_token = response.get('nextPageToken', None)\n",
        "            if page_token is None:\n",
        "                break\n",
        "\n",
        "    except HttpError as error:\n",
        "        print(F'An error occurred: {error}')\n",
        "        files = None\n",
        "\n",
        "    return files\n"
      ],
      "metadata": {
        "id": "GoFYxJGlUJb6"
      },
      "execution_count": 300,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_spreadsheet(sheet_title, output_folder_id):\n",
        "  \"\"\"\n",
        "  Create a new spreadsheet with the given title (sheet_title). Can be saved in a specific folder by giving the folder_id in a list\n",
        "  \"\"\"\n",
        "  drive_api = build('drive', 'v3', credentials=creds)\n",
        "\n",
        "  print(\"Creating Sheet %s\", sheet_title)\n",
        "  body = {\n",
        "      'name': sheet_title,\n",
        "      'mimeType': 'application/vnd.google-apps.spreadsheet',\n",
        "  }\n",
        "\n",
        "  if output_folder_id:\n",
        "      body[\"parents\"] =  output_folder_id\n",
        "\n",
        "  req = drive_api.files().create(body=body)\n",
        "  new_sheet = req.execute()\n",
        "  \n",
        "  return new_sheet[\"id\"]"
      ],
      "metadata": {
        "id": "rf-3kh6yz0ZP"
      },
      "execution_count": 301,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/a/2556252/13590692\n",
        "def rreplace(s, old, new, occurrence):\n",
        "  \"\"\"\n",
        "  Replace last x occurences of a substring in a string with a new substring\n",
        "  \"\"\"\n",
        "  li = s.rsplit(old, occurrence)\n",
        "  return new.join(li)"
      ],
      "metadata": {
        "id": "7CjKgFNxdP48"
      },
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_average_worksheet(spreadsheet, worksheet_names):\n",
        "  \"\"\"\n",
        "  Generates the worksheet displaying the averages of each entity over the pages\n",
        "  \"\"\"\n",
        "  entities = []\n",
        "  formulas = []\n",
        "  for entity in cell_positions_of_averages:\n",
        "    entities.append(entity)\n",
        "    formula = \"=AVERAGE(\"\n",
        "    for name in worksheet_names:\n",
        "      formula = F\"{formula}'{name}'!{cell_positions_of_averages[entity]};\"\n",
        "    formula = rreplace(formula, ';', '', 1)\n",
        "    formulas.append(F\"{formula})\")\n",
        "  \n",
        "  worksheet_content = {\"Entity\": entities, \"F1\": formulas}\n",
        "  \n",
        "  worksheet = spreadsheet.add_worksheet(title=\"Average\", rows=len(entities), cols = len(worksheet_content))\n",
        "  gd.set_with_dataframe(worksheet, pd.DataFrame(data=worksheet_content))"
      ],
      "metadata": {
        "id": "8ZRfVK8WW4IW"
      },
      "execution_count": 303,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_calculations(worksheet):\n",
        "  \"\"\"\n",
        "  Add the calculations for the F1 score to a worksheet (based on a given template)\n",
        "  \"\"\"\n",
        "  worksheet_df = pd.DataFrame(worksheet.get_all_records())\n",
        "\n",
        "  for column in calculation:\n",
        "    if column in worksheet_df:\n",
        "      print(\"Worksheet has same columns as calculations, will not be updated for: \" + row)\n",
        "    else:\n",
        "      worksheet_df[column] = calculation[column]\n",
        "\n",
        "  if (len(worksheet_df) < len(calculation)):\n",
        "    difference = len(worksheet_df) - len(calculation)\n",
        "    for i in range(len(worksheet_df), len(calculation)):\n",
        "      worksheet_df = worksheet_df.append(calculation.iloc[i], ignore_index=True)\n",
        "  \n",
        "  gd.set_with_dataframe(worksheet, worksheet_df)"
      ],
      "metadata": {
        "id": "o1u9C43qqDrJ"
      },
      "execution_count": 304,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_csv_files_to_spreadsheet(files, spreadsheet):\n",
        "  # Generating a list of the sheet names saves API calls (less risk of exceeding the quota)\n",
        "  worksheet_names = list()\n",
        "\n",
        "  for file in files:\n",
        "    filename = file.get(\"name\")\n",
        "    sheetName = filename.split(\".\")[0]\n",
        "    sheet_id = file.get(\"id\")\n",
        "    downloaded_content = gdrive.CreateFile({'id' : sheet_id})\n",
        "    downloaded_content.GetContentFile(filename) \n",
        "    content_df = pd.read_csv(filename)\n",
        "    no_rows = len(content_df)\n",
        "    no_cols = len(content_df.columns)\n",
        "    \n",
        "    worksheet = spreadsheet.add_worksheet(title=sheetName, rows = no_rows, cols = no_cols)\n",
        "    gd.set_with_dataframe(worksheet, content_df)\n",
        "    worksheet = add_calculations(worksheet)\n",
        "    worksheet_names.append(sheetName)\n",
        "\n",
        "  # Delete Fragment from creation\n",
        "  spreadsheet.del_worksheet(spreadsheet.sheet1)\n",
        "\n",
        "  # Add average worksgeet\n",
        "  add_average_worksheet(spreadsheet, worksheet_names)\n",
        "  \n",
        "  return worksheet_names"
      ],
      "metadata": {
        "id": "PCYiEqjspRmQ"
      },
      "execution_count": 305,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_spreadsheet_from_folder(input_folder_id):\n",
        "  \"\"\"\n",
        "  Move all CSV files in a folder into one spreadsheet\n",
        "  \"\"\"\n",
        "  files = search_file(input_folder_id, \"text/csv\")\n",
        "  \n",
        "  if len(files) <= 0:\n",
        "    print(\"No csv files found\")\n",
        "    return\n",
        "\n",
        "  spreadsheet_name = files[0].get(\"name\").split(\"-\")[0]\n",
        "  parent_list = [output_folder]\n",
        "  spread_id = create_spreadsheet(spreadsheet_name, parent_list)\n",
        "\n",
        "  spreadsheet = gc.open_by_key(spread_id)\n",
        "\n",
        "  worksheets = add_csv_files_to_spreadsheet(files, spreadsheet)"
      ],
      "metadata": {
        "id": "8BZkcmoXFgVl"
      },
      "execution_count": 306,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iterate_over_folders(directory_id):\n",
        "  \"\"\"\n",
        "  Find all folders in a given subdirectory and iterate over them to move the csv files of each folder in their separate spreadsheets.\n",
        "  \"\"\"\n",
        "  folders = search_file(directory_id, \"application/vnd.google-apps.folder\")\n",
        "  for folder in folders:\n",
        "    generate_spreadsheet_from_folder(folder.get(\"id\"))\n",
        "\n",
        "    print(\"Initiating wait time of 20 seconds as to not exceeding the limit of requests.\")\n",
        "    time.sleep(20)"
      ],
      "metadata": {
        "id": "bjFqhEX5UMdC"
      },
      "execution_count": 307,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iterate_over_folders(search_folder_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhTVkXtkkc9w",
        "outputId": "79b39ee6-297a-461d-f039-9d3642df1371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found: 10, 129Kw66lwaRzd-Q2QzRcBBlXAa1ADGDw_\n",
            "Found: 1000, 1Y-mF6iZ003KvDd74Slkx407K4lQ8sIBv\n",
            "Found: 100, 1Q1MqauGNsgTRMVk1cgu3zP1LHExsrINA\n",
            "Found: 250, 1J5FaUBF79LtOhbga857bTc0PVCbJcwha\n",
            "Found: 50, 1rx-GrtP9c41aHz6Jr410cqiHxOSw5NBI\n",
            "Found: 500, 1DnegjId99ZdjejR7RqjKzusMHAkG62C6\n",
            "Found: short_name_templates10-4.csv, 1UglGkVJD-jeMbkJ8KE_EVMBZ4YPo1SwQ\n",
            "Found: short_name_templates10-3.csv, 1EyqK6VXpVk5uuDMyUzjRNZI7Qg3ws3GM\n",
            "Found: short_name_templates10-2.csv, 1mFdTPJNNTtTd3UF3-AoRFnCA_OSomwzY\n",
            "Found: short_name_templates10-1.csv, 1vsxYSdWerI0bhc9IUlbXeWIha401GTFg\n",
            "Found: short_name_templates10-0.csv, 11mmTk9eZTJaZAbQqsu1sVz_wjGa7_C8M\n",
            "Creating Sheet %s short_name_templates10\n",
            "Initiating wait time of 20 seconds as to not exceeding the limit of requests.\n",
            "Found: short_name_templates1000-4.csv, 1hkK_LJfS91mD_EOj4deKTPILvEwRrGWT\n",
            "Found: short_name_templates1000-3.csv, 1Cm0JtAiZqPui9Gtag0k7_Sl3udxAPKCa\n",
            "Found: short_name_templates1000-2.csv, 1lY6tn9sS_bc4rfVBqm1w2ai8XvFInFkI\n",
            "Found: short_name_templates1000-1.csv, 16hjtlKQWFlF8KroHO_SCmLHyUFQ9da0i\n",
            "Found: short_name_templates1000-0.csv, 1rAgzp614_NTO3f02EbAoLmnRBvveWx1B\n",
            "Creating Sheet %s short_name_templates1000\n",
            "Initiating wait time of 20 seconds as to not exceeding the limit of requests.\n",
            "Found: short_name_templates100-4.csv, 1W-0Xwn7wnilVQUf1yDmjIAR0FBEp-T1a\n",
            "Found: short_name_templates100-3.csv, 1x4vLbL8fwNNzmMfdpCyzLFI17qe2MNC5\n",
            "Found: short_name_templates100-2.csv, 1rEufzM4m59ZkLhv584HGi51lT6UhGXry\n",
            "Found: short_name_templates100-1.csv, 18FQCyKW98mvZR9TlqTlbjc_kq5UDojQp\n",
            "Found: short_name_templates100-0.csv, 1SFf_BDuXFy4o5re5KP0JPPuonHBnxh3x\n",
            "Creating Sheet %s short_name_templates100\n",
            "Initiating wait time of 20 seconds as to not exceeding the limit of requests.\n",
            "Found: short_name_templates250-4.csv, 1aA4a50Tk_WvXqk_OvIW7YIipOkKNpVn7\n",
            "Found: short_name_templates250-3.csv, 1c2lQfwkLCr6lpTKU92goi4wi0Sg8oRCz\n",
            "Found: short_name_templates250-2.csv, 15usZ3GKHi0DolsxHdQP1E7OqMl85pk0V\n",
            "Found: short_name_templates250-1.csv, 1Km7-EFSv6LPgwd-pLJ9WeWVR183493rS\n",
            "Found: short_name_templates250-0.csv, 1Oopr33xmJnpJz1wxPDO-vO-8nX758HZx\n",
            "Creating Sheet %s short_name_templates250\n"
          ]
        }
      ]
    }
  ]
}